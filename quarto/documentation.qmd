---
title: "Marketplace Boost Optimization Engine"
author: "Felipe Osorio"
date: "April 4, 2025"
format: 
  revealjs:
    incremental: false
editor: visual
---

```{r setup, include=FALSE}
options(scipen=5)
setwd('~/Desktop/code/boost_optimization/quarto/')
source('../r/01_transform_data.R')
library(tidyverse)
library(ggplot2)

metros_top6 = c("Orlando Metro", "DFW", "Sacramento Metro", "Greensboro Metro", "Worcester Metro", "Hampton Roads")
```

## The Problem

The current strategy of augmenting base fares with incentives is inefficient on two fronts:

-   It costs our teams valuable time to manually intervene in the marketplace, i.e. "boosting"

-   Boosting trips is expensive! As we try to guarantee every ride, boost costs can accumulate on unclaimed rides leading up to their scheduled start

## The Problem (don't panic)

```{r}
ggplot(df_boosted, aes(x=time_between_boost_scheduled_h*-1, y = single_boost_amount_cents/100)) + 
  geom_point(alpha=0.3) + 
  ylim(c(-6,40)) + 
  stat_smooth(se=T, col="red", lwd=1.5) +
  xlab("Time until scheduled pick-up, in hours") +
  ylab("Single Boost Amount ($)")
```

## The Problem, quantified

**In 2024, we applied 11,800 boosts from May-August**

-   Let's suppose each intervention takes just six minutes to identify, price out, decide on, and execute
    -   Assuming boosting isn't yet automated, and including "zero cent" boosts
-   **In May alone, we needed 700+ people-hours for 7,135 boosts that could have been spent elsewhere**

We can do better than that!

## The Problem, quantified (\$)

**In 2024, we boosted 42.7% of all completed trips (17,500)**

-   In May - August, the company paid \$430k to drivers

-   The avg. trip was 21 minutes; the avg. fare was \$25

-   We spent \$92k on boosting, or \$5.27 per trip (21% of fares)

-   **Boosts made up over a fifth of total fares paid to drivers**

That's \$280k in annualized boost spend, under the status quo

## The Data

```{r}
ggplot(df_clean, aes(x=dollars_paid_to_driver)) + geom_histogram() + xlab("Fare paid to driver ($)") + facet_wrap(~origin_metro_area_name, scales="free_y") + ggtitle("Not all metros are created equal")
```

#### Histograms of fares paid to drivers, by origin metro area

## The Data

Let's zoom in on the timeline of driver claims versus scheduled start times

```{r}
ggplot(df_clean, aes(x=time_between_claimed_scheduled_h*-1)) + geom_histogram() + xlab("Time between a claimed trip and its scheduled start, in hours")
```

-   Many trips are claimed several weeks or days in advance, but some just hours before. A third (33.2%) of trips remained unclaimed an hour before pick-up!

## The Data (features)

Beyond **fares, boosts, claimed/scheduled ride times**, we have also collected information on:

-   **Peak traffic hours**

-   **Total predicted trip distance & duration**

-   **Boosts expressed as a percentage of fares**

-   **\# of trips by origin metro area**, monthly (demand proxy)

-   **Average commute time to pick-up by metro area**, also monthly (sparsity proxy)

## Behold, a solution: Identifying high-risk rides early

I propose leveraging a predictive model to automatically flag rides many days in advance, particularly those likely to require a last-minute intervention

-   **Key Question:** will the requisite data become available ahead of each trip's scheduled date?
-   If we can probabilistically identify rides at high risk of remaining unclaimed, we can promote them on the platform early – saving on last-minute boosting
    -   A quarter of overall boost spend happens in the final hour

## A model-based approach to the business problem

In developing a model, we seek to balance three high-level strategic goals:

-   **Reducing** the manual workload on boost ops teams as much as possible
-   **Maintaining** the high standard of scheduled trip reliability and guaranteed rides
-   **Minimizing** the total "dollar pool" allocated to boosting (i.e. spend)

## A model-based approach to the business problem

I fit a predictive model to measure the extent to which our input features influenced each trip's timeline

-   Specifically, what is the relationship between our variables of interest and the time until a trip gets claimed (relative to its scheduled start), in hours?

-   Let's start with a relatively simple linear model (GLM)

## The Model: How the sausage is made

```{r}
source('../r/03_fit_toy_model.R')
## Honestly not a great fit, but it's a good start!
```

## The Model: A technical detour

```{r}
ggplot(df_clean, aes(x = residuals)) + geom_histogram(bins = 30) + xlab("Residuals") + ylab("")
```

#### Residuals Plot (observed - predicted values)

## The Model: A technical detour

-   On the plus side, residuals are centered on zero (mu = 0.003)

-   Yet, we observe an asymmetric distribution and a high standard deviation among residuals (sd = 149 h), suggesting distributional assumptions don't hold up

-   In brief, my toy model's fit and performance are lackluster

## The Model: A technical detour

```{r}
ggplot(df_clean, aes(x=log1p(time_between_claimed_scheduled_h))) + geom_histogram() + xlab("Time between a claimed trip and its scheduled start, log hours")
```

-   During the modeling phase, I specified a log link function for the outcome variable

-   It was insufficient to accommodate the degree of non-normality, evidenced by this right-skew

## The Model: A technical detour

-   A modeling approach requires an understanding of the data-generating process, checking for the influence of outliers, and iterating on the implementation details

-   If this were a real-world deliverable, we would consider:

    -   A different family of continuous distributions to model the problem, or

    -   A binary outcome for a logistic model: e.g. binary classification predicting high vs. low risk trips

-   For example, a zero-inflated negative binomial distribution could better capture the high frequency of last-minute claimed trips

## Interpreting model results

Despite the toy model's shortcomings, this framework remains useful for making actionable recommendations. Controlling for the input variables, and all else being equal, some insights:

-   **Longer rides are likely to get claimed earlier**

-   **High boosts** (by % of fare) are **strongly associated** with late trip claims

-   Higher fare and peak hour trips tend to get claimed later

-   Both geographic-level features tracking monthly rides & commute times are modestly related to later claims

## Toy model deployment

If we were satisfied with model fit and diagnostics, I'd work on deploying it with the MLOps team, with the following principles in mind:

-   **Modularity**: each component of the modeling pipeline should be easy to maintain/improve
-   **Efficiency**: minimize tech debt, ci/cd build times, and model run-time for scaling out
-   **Reproducibility**: a containerized environment (e.g. docker image) to make it portable
-   **Generalizability**: predictions should work well with new trips data, e.g. 2025 metro areas

## Bottom-line Recommendations (pt. 1)

Embracing a hybrid predictive and prescriptive framework, my business recommendations include:

-   **Incentivizing drivers early** with a probabilistic, automated boosting system, based on the model
-   **Offering** **boosts as a percentage of total fare**, to minimize the need for incremental boosts
-   Making incentives proportional to trips' predicted risk of remaining unclaimed until late

## Bottom-line Recommendations (pt. 2)

Following a recommendations testing framework, we should also consider:

-   **Highlighting above-average boosts** in the app when they're offered days in advance
-   **Adding an "early claim" bonus** for drivers, e.g. every five trips claimed 8+ hours early
-   **Designing a set of experiments** to understand how all our proposed changes will interact

## Business Impact (if you read 1 slide)

-   Same-day boosting costs \$5.27 per trip, on average

    -   A quarter of boosts (\$\$) happen within an hour of the pick-up window!

-   Using predictive modeling, **we can identify rides that might be left unclaimed** weeks in advance

-   Automating early boosting would save both time and money

    -   A modest 19% reduction in average boost spend works out to \$53k/yr (of \$280k in boosts)

    -   Put another way, **a boost reduction of \$1 per \$25 trip saves the business 4% on every fare**

## If only we had more data...

While this is fine for a POC, we could really use more data:

-   **More trips** – we're clearly working with a small sample
-   Competitive **pricing benchmarks** from other transportation networks, by metro area
-   Twelve or more months of **historical data for seasonality** & cyclicality estimates
-   **Canceled and rescheduled trips** (or those we couldn't guarantee), another form of risk
-   A more detailed history of "unclaimed trip" events, so that we can predict those too

## Limitations (information horizon)

Data aren't the only limiting factor. My approach makes several key assumptions:

-   The framework **assumes data inputs would be available early enough for the information to make a difference** for the business, well before same-day boost ops kicks in

-   We're also **assuming knowledge on fares paid to drivers** ahead of time. Shifts in the predicted trip length/duration could systematically impact model results

-   **YMMV**: we need a framework for quantifying and tracking the value-add once implementated

## Limitations ("out-of-sample")

-   We've operated based on "within-sample" data, and wish to understand how well results would generalize to other times of the year, geographic regions, and future trips in general

    -   We can run K-fold cross validation (e.g. 10 splits), ideally with a full year's worth of data

-   We should develop an A/B testing framework for comparing boost optimization and other incentive-based strategies

-   In the future, we may consider a time-to-event (i.e. survival analysis) or zero-inflated overdispersion model, such as a negative binomial HLM (using `rstan`) to predict trip claims

## 
