---
title: "Marketplace Boost Optimization Engine"
author: "Felipe Osorio"
date: "March 17, 2025"
format: 
  revealjs:
    incremental: false
editor: visual
---

```{r setup, include=FALSE}
options(scipen=5)
setwd('~/Desktop/code/boost_optimization/quarto/')
source('../r/01_transform_data.R')
library(tidyverse)
library(ggplot2)

metros_top6 = c("Orlando Metro", "DFW", "Sacramento Metro", "Greensboro Metro", "Worcester Metro", "Hampton Roads")
```

## The Problem

The current strategy of augmenting base fares with incentives ("boosts") is inefficient on two fronts:

-   It costs us valuable time to manually intervene in the scheduled trips marketplace

-   Boosting trips is expensive, and as we try to guarantee every ride, they get additional increments near the pick-up window if they remain unclaimed

## The Problem (don't panic)

```{r}
ggplot(df_boosted, aes(x=time_between_boost_scheduled_h*-1, y = single_boost_amount_cents/100)) + 
  geom_point(alpha=0.3) + 
  ylim(c(-6,40)) + 
  stat_smooth(se=T, col="red", lwd=1.5) +
  xlab("Time until scheduled pick-up, in hours") +
  ylab("Single Boost Amount ($)")
```

## The Problem, quantified

In 2024, 11,800 boosts were applied from May-August

-   Let's assume each intervention takes just six minutes to identify, price out, decide on, and execute

<!-- -->

-   Including those that receive a zero cent value...

-   **In May, applying 7,135 boosts required 700+ people-hours that should ideally be spent elsewhere**

We can do better than that!

## The Problem, quantified

**In 2024, 42.7% of completed trips (17,500) needed a boost**

-   Between May - August, we paid out \$430k to drivers

-   The avg. trip was 21 minutes, and the avg. fare \$25

-   During that period, we spent \$92k on boosting (21% of fares)

-   **Boosts made up over a fifth of total fares paid to drivers**

That's \$280k in annualized spend, under the status quo

## The Data

```{r}
ggplot(df_clean, aes(x=dollars_paid_to_driver)) + geom_histogram() + xlab("Fare paid to driver ($)") + facet_wrap(~origin_metro_area_name, scales="free_y")
```

#### Histogram of fares paid to drivers, by origin metro area

## The Data

If you look closely at the timeline of driver claims versus their trips' scheduled start times, things start to get messy...

```{r}
ggplot(df_clean, aes(x=time_between_claimed_scheduled_h*-1)) + geom_histogram() + xlab("Time between a claimed trip and its scheduled start, in hours")
```

-   Many trips are claimed weeks in advance, but some only days or even hours before. About a third (33.2%) of trips remain unclaimed an hour before pick-up!

## The Data (features)

In addition to fares, boosts, claims, and scheduled ride times, we've also collected information on:

-   Peak traffic hours

-   Total predicted trip distance & duration

-   Boosts expressed as a percentage of driver fare

-   Count of trips by origin metro area, monthly (demand proxy)

-   Average commute time to pick-up by metro area, also monthly (traffic/sparsity proxy)

## Behold, a solution: Identifying high-risk rides early

My solution involves using a predictive model to automatically flag rides days in advance, particularly those at risk of needing a last-minute intervention

-   We must know whether the requisite data are available well in advance of each trip's scheduled start

If we can probabilistically identify rides that are most likely to remain unclaimed at the eleventh hour, we can promote them on the platform early – and time is money.

## A model-based approach to the business problem

In developing a model, our high-level goals include:

-   **Reducing** the manual workload on boost ops teams as much as possible
-   **Maintaining** the high standard of scheduled trip reliability and guaranteed rides
-   **Minimizing** the total "dollar pool" allocated to boosting (i.e. spend)

## A model-based approach to the business problem

We fit a model to measure the extent to which our data points influence each claimed trip's timeline

-   Specifically, how is each feature associated with the number of hours until a scheduled ride gets claimed?

-   Let's start with a relatively simple linear model (GLM)

## The Model: How the sausage is made

```{r}
source('../r/03_fit_toy_model.R')
## Honestly not a great fit, but it's a good start!
```

## The Model: Technical detour

```{r}
ggplot(df_clean, aes(x = residuals)) + geom_histogram(bins = 30) + xlab("Residuals") + ylab("")
```

#### Residuals Plot (observed - predicted values)

## The Model: Technical detour

On the positive side, the residuals are centered on zero

Yet, we observe an asymmetric distribution of residuals and a high standard deviation among them (sd = 149 h), suggesting distributional assumptions are not holding up

In brief, our toy model's fit and performance leave much to be desired

## The Model: Technical detour

```{r}
ggplot(df_clean, aes(x=log1p(time_between_claimed_scheduled_h))) + geom_histogram() + xlab("Time between a claimed trip and its scheduled start, log hours")
```

-   During the modeling phase, we specified a log link function for the outcome variable

-   This was insufficient to accommodate the degree of non-normality, evidenced by the right-skew

## The Model: Technical detour

-   A modeling approach requires an understanding of the data-generating process, defining and checking for the influence of outliers, and iterating on the implementation details

-   If this were a real product, we'd consider either:

    -   A different family of continuous distributions to model the problem

    -   An alternative outcome for a logistic model: e.g. binary response predicting high vs. low risk trips

-   Another idea: a zero-inflated negative binomial distribution could better capture the high frequency of last-minute claimed trips

## Interpreting model results

Despite our toy model's shortcomings, the framework remains useful for making actionable recommendations. Controlling for all other inputs, and all else being equal, several insights:

-   Longer rides are likely to get claimed earlier

-   Both higher fare rides and peak hour trips tend to get claimed later

-   High boosts (by % of fare) are strongly associated with late trip claims

-   Both geo-based indicators of monthly rides & commute times are modestly associated with later claims

## Toy model deployment

**If we were** **satisfied** with model fit and diagnostics, we'd work on deploying it with the MLOps team, keeping these principles in mind:

-   **Modularity**: each component of the modeling pipeline should be easy to maintain/improve
-   **Efficiency**: minimize tech debt, ci/cd build times, and model run-time for scaling out
-   **Reproducibility**: a containerized environment (e.g. docker image) to make it portable
-   **Generalizability**: predictions should work well with new trips data, e.g. 2025 metro areas

## Bottom-line Recommendations (pt. 1)

Embracing a hybrid predictive and prescriptive framework, my business recommendations include:

-   Incentivizing drivers early with a probabilistic, automated boosting system, leveraging the model
-   Offering incentives as a percentage of total fare, to minimize the need for incremental boosts
-   Making boosts proportional to trips' predicted risk of remaining unclaimed on each day

## Bottom-line Recommendations (pt. 2)

Following a recommendations testing framework, we should also consider:

-   Highlighting above-average boosts in the app when they're offered days in advance
-   Adding an "early claim" bonus for drivers, e.g. every five rides claimed 6+ hours early
-   Designing a set of experiments to understand how all the proposed changes might interact

## Business Impact (if you read 1 slide)

-   Last-minute boosting costs the business \$5.27 per trip

    -   A quarter of boosts (\$\$) happen within an hour of pick-up!

-   Using predictive modeling, we can identify rides that may be left unclaimed many days in advance

-   Automating early boosting for these trips will save the business time and money

    -   A modest 19% reduction in boost spend will save us \$53k/yr (of \$280k total fares)

    -   Put another way, an average boost reduction of \$1 per \$25 trip saves us 4% on every fare

## If only we had more data...

While this is fine for a POC, we could really use more data:

-   More trips – we're clearly working with a small sample here
-   Competitive pricing benchmarks from other transportation networks, by metro area
-   Twelve or more months of historical data for seasonality & cyclicality estimates
-   Canceled and rescheduled trips (or those we couldn't guarantee), another form of risk
-   A detailed history of "unclaimed trip" events, so that we can predict those too

## Limitations (information horizon)

Data aren't the only limiting factor. My approach comes with a set of key assumptions:

-   The framework assumes each input would be present early enough so that the information would make a difference for the business, long before manual boosts kick in

-   We're assuming we know how much the driver will be paid ahead of time. Also, if the trip length/duration predictions shift, it could systematically impact model results

-   YMMV: we need a way to quantify and track the real value-add for the business once implementated

## Limitations ("out-of-sample")

-   We've operated based on within-sample data, and need to understand how well results might generalize

    -   We should run K-fold cross validation (e.g. 10 splits), ideally with a full year's worth of data

-   We should develop an A/B testing framework for comparing boost optimization and other incentive-based strategies

-   In the future, we may consider a time-to-event (i.e. survival analysis) or zero-inflated overdispersion model, such as a negative binomial HLM (using `rstan`) to predict trips

## 
